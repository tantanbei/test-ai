{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ad7bb6",
   "metadata": {},
   "source": [
    "访问本地ollama模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cb99e-cb87-42d5-ba3c-bc4935737a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# 选择 LLM，如 \"mistral\" 或 \"llama2\"\n",
    "llm = Ollama(model=\"deepseek-r1:14b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# 生成文本\n",
    "response = llm(\"你是谁\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40270b0",
   "metadata": {},
   "source": [
    "访问本地ollama模型+资料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601bdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#公共结构定义\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# 根据文档类型选择参数\n",
    "splitter_configs = {\n",
    "        \"general\": {\n",
    "            \"chunk_size\": 1000,\n",
    "            \"chunk_overlap\": 200\n",
    "        },\n",
    "        \"technical\": {\n",
    "            \"chunk_size\": 1500,\n",
    "            \"chunk_overlap\": 300,\n",
    "            \"separator\": \"\\n\\n\"\n",
    "        },\n",
    "        \"code\": {\n",
    "            \"chunk_size\": 800,\n",
    "            \"chunk_overlap\": 100,\n",
    "            \"separator\": \"\\n\\n\"\n",
    "        },\n",
    "        \"markdown\": {\n",
    "            \"chunk_size\": 1000,\n",
    "            \"chunk_overlap\": 150,\n",
    "            \"separator\": \"\\n## \"    # Markdown 二级标题作为分隔符\n",
    "        },\n",
    "        \"news\": {\n",
    "            \"chunk_size\": 500,      # 减小块大小\n",
    "            \"chunk_overlap\": 50,     # 适当的重叠\n",
    "            \"separator\": \"\\n\"        # 使用换行符作为分隔符\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 文件类型到加载器的映射\n",
    "LOADER_MAPPING = {\n",
    "    '.txt': TextLoader,\n",
    "    '.md': UnstructuredMarkdownLoader,\n",
    "    '.pdf': PyPDFLoader,\n",
    "    '.csv': CSVLoader,\n",
    "    '.json': JSONLoader\n",
    "}\n",
    "\n",
    "class NormalizedOllamaEmbeddings(OllamaEmbeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = super().embed_documents(texts)\n",
    "        # L2 归一化\n",
    "        normalized = [embedding/np.linalg.norm(embedding) for embedding in embeddings]\n",
    "        return normalized\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        embedding = super().embed_query(text)\n",
    "        # L2 归一化\n",
    "        return embedding/np.linalg.norm(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_documents_from_directory(\n",
    "    directory_path: str,\n",
    "    db_path: str,\n",
    "    doc_type: str = \"news\",\n",
    "    supported_extensions: List[str] = ['.txt', '.md', '.py', '.pdf']\n",
    ") -> None:\n",
    "    # 设置嵌入模型\n",
    "    embedding = NormalizedOllamaEmbeddings(\n",
    "        model=\"bge-m3:latest\",\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "    \n",
    "    # 获取分割器配置\n",
    "    splitter_config = splitter_configs.get(doc_type, splitter_configs[\"general\"])\n",
    "    text_splitter = CharacterTextSplitter(**splitter_config)\n",
    "    \n",
    "    # 存储所有文档\n",
    "    all_documents = []\n",
    "    file_count = 0\n",
    "    \n",
    "    try:\n",
    "        # 遍历目录\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in supported_extensions):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        # 添加调试信息\n",
    "                        print(f\"\\n开始处理文件: {file_path}\")\n",
    "                        print(f\"文件大小: {os.path.getsize(file_path)} bytes\")\n",
    "                        \n",
    "                        # 尝试直接读取文件内容\n",
    "                        # with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        #     raw_content = f.read()\n",
    "                        #     print(f\"原始文件内容长度: {len(raw_content)} 字符\")\n",
    "                        \n",
    "                        # 加载文档\n",
    "                        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                        if file_extension in LOADER_MAPPING:\n",
    "                            loader_class = LOADER_MAPPING[file_extension]\n",
    "                            loader = loader_class(file_path)  # 显式指定编码\n",
    "                            documents = loader.load()\n",
    "                            print(f\"加载后的文档数量: {len(documents)}\")\n",
    "                            \n",
    "                            if len(documents) == 0:\n",
    "                                print(f\"警告: 文档加载后为空\")\n",
    "                                continue\n",
    "                                \n",
    "                            # 分割文档\n",
    "                            texts = text_splitter.split_documents(documents)\n",
    "                            print(f\"分割后的文本块数量: {len(texts)}\")\n",
    "                            \n",
    "                            if len(texts) == 0:\n",
    "                                print(f\"警告: 文本分割后为空\")\n",
    "                                continue\n",
    "                                \n",
    "                            all_documents.extend(texts)\n",
    "                            file_count += 1\n",
    "                            print(f\"成功加载文件: {file_path}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "                        print(f\"错误类型: {type(e).__name__}\")\n",
    "                        import traceback\n",
    "                        print(f\"详细错误信息: {traceback.format_exc()}\")\n",
    "                        \n",
    "        print(f\"\\n总共处理了 {file_count} 个文件\")\n",
    "        print(f\"总共获得 {len(all_documents)} 个文本块\")\n",
    "        \n",
    "        # # 创建或更新向量数据库\n",
    "        # try:\n",
    "        #     # 尝试加载现有数据库\n",
    "        #     vector_db = FAISS.load_local(\n",
    "        #         db_path,\n",
    "        #         embeddings=embedding,\n",
    "        #         allow_dangerous_deserialization=True\n",
    "        #     )\n",
    "        #     print(\"加载现有数据库成功，添加新文档...\")\n",
    "        #     vector_db.add_documents(all_documents)\n",
    "        # except:\n",
    "        print(\"创建新的向量数据库...\")\n",
    "        print(all_documents)\n",
    "        vector_db = FAISS.from_documents(all_documents, embedding)\n",
    "        \n",
    "        # 保存数据库\n",
    "        vector_db.save_local(db_path)\n",
    "        print(\"成功保存向量数据库\")\n",
    "        \n",
    "        # 测试搜索\n",
    "        results = vector_db.similarity_search(\"黄金\", k=7)\n",
    "        print(\"\\n测试搜索结果:\")\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"\\n文档 {i}:\")\n",
    "            print(f\"来源: {doc.metadata['source']}\")\n",
    "            print(f\"内容: {doc.page_content[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中出错: {e}\")\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "directory_path = \"test_file\"  # 文档目录\n",
    "doc_type = \"technical\"        # 文档类型\n",
    "supported_extensions = [      # 支持的文件类型\n",
    "    '.txt',\n",
    "    '.md',\n",
    "    '.pdf',\n",
    "    '.json',\n",
    "    '.csv'\n",
    "]\n",
    "    \n",
    "# 执行导入\n",
    "load_documents_from_directory(\n",
    "    directory_path=directory_path,\n",
    "    db_path=\"faiss_index\",\n",
    "    doc_type=doc_type,\n",
    "    supported_extensions=supported_extensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试txt文件向量化\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 设置嵌入模型\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"bge-m3:latest\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# 加载文档\n",
    "loader = TextLoader('test_file/金价向上、金店向下：金饰品牌加速关店.txt')  # 替换为你的文档路径\n",
    "documents = loader.load()\n",
    "\n",
    "# 分割文档\n",
    "# 根据文档类型选择参数\n",
    "splitter_config = splitter_configs.get(\"news\", splitter_configs[\"general\"])\n",
    "print(splitter_config)\n",
    "\n",
    "# 创建分隔器\n",
    "text_splitter = CharacterTextSplitter(**splitter_config)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(texts)\n",
    "\n",
    "# 创建向量数据库\n",
    "vector_db = FAISS.from_documents(texts, embedding)\n",
    "\n",
    "# 保存到本地（注意路径）\n",
    "vector_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试pdf文件向量化\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 设置嵌入模型\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=\"bge-m3:latest\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# 加载文档\n",
    "loader = PyPDFLoader('test_file/test_pdf.pdf')  # 替换为你的文档路径\n",
    "documents = loader.load()\n",
    "\n",
    "# 分割文档\n",
    "# 根据文档类型选择参数\n",
    "splitter_config = splitter_configs.get(\"technical\", splitter_configs[\"general\"])\n",
    "print(splitter_config)\n",
    "\n",
    "# 创建分隔器\n",
    "text_splitter = CharacterTextSplitter(**splitter_config)\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(texts)\n",
    "\n",
    "# 创建向量数据库\n",
    "vector_db = FAISS.from_documents(texts, embedding)\n",
    "\n",
    "# 保存到本地（注意路径）\n",
    "vector_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb563940-57c0-4c93-a133-94c7ffd8cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试faiss搜索\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "# 使用 Ollama 的嵌入模型\n",
    "embedding = NormalizedOllamaEmbeddings(\n",
    "    model=\"bge-m3:latest\",  # 或其他你想使用的模型\n",
    "    base_url=\"http://localhost:11434\"  # 使用你的 Ollama 服务端口\n",
    ")\n",
    "\n",
    "# 创建或加载向量数据库\n",
    "vector_db = FAISS.load_local(\n",
    "    \"faiss_index\", \n",
    "    embeddings=embedding,\n",
    "    allow_dangerous_deserialization=True  # 添加这个参数\n",
    ")\n",
    "\n",
    "# 语义搜索\n",
    "query = \"rank表是什么\"\n",
    "# 增加 k 和 score_threshold\n",
    "docs_and_scores = vector_db.similarity_search_with_score(\n",
    "    query,\n",
    "    k=7,                    # 返回更多结果\n",
    "    score_threshold=0.9     # 设置相似度阈值\n",
    ")\n",
    "\n",
    "print(\"\\n=== 搜索结果 ===\")\n",
    "for i, (doc, score) in enumerate(docs_and_scores, 1):\n",
    "    print(f\"\\n文档 {i} (相似度分数: {score}):\")\n",
    "    print(f\"来源: {doc.metadata['source']}\")\n",
    "    print(f\"内容: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04536e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#直接使用 LLM + 模板\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 处理搜索结果\n",
    "def get_answer(query, search_results):\n",
    "    # 合并上下文\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in search_results])\n",
    "    \n",
    "    # 生成完整提示\n",
    "    final_prompt = prompt.format(\n",
    "        context=context,\n",
    "        question=query\n",
    "    )\n",
    "    \n",
    "    # 获取回答\n",
    "    for chunk in llm.stream(final_prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# 创建 LLM\n",
    "llm = Ollama(\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    ")\n",
    "\n",
    "# 创建提示模板\n",
    "template = \"\"\"基于以下信息回答问题：\n",
    "\n",
    "背景信息：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "请给出详细的回答：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "query = \"rank表是什么\"\n",
    "\n",
    "get_answer(query, vector_db.similarity_search(query, k=7, score_threshold=0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
